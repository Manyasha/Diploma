\chapter{Методи нелінійного програмування без обмежень}

\section {Постановка задачі нелінійного програмування}

У загальному вигляді математична модель задачі нелінійного програмування формулюється наступним чином: знайти вектор , $X = (x_1, x_2, \ldots, x_n) $ при якому б цільова функція набувала свого екстремального (максимального чи мінімального) значення (можливо лише в тому випадку коли цільова функція неперервна, а допустима множина розв'язків замкнена, непуста і обмежена):
$$ \max(\min) \Phi = \phi (x_1, x_2, \ldots, x_n) \eqno(1.1.1) $$
при наступних обмеженнях:
$$ 
\begin{cases}
g_1(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_1 &  \\
g_2(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_2 &  \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots & \\ 
g_m(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_m &  \\
\end{cases}	\eqno(1.1.2)
$$
$$
x_j \geq 0, \; (j = \overline{1,n}) \eqno(1.1.3)
$$
де $ x_j \; (j = \overline{1,n}) $ - змінні \\
$\phi (x_1, x_2, \ldots, x_n)$ - нелінійна функція від $n$ змінних, \\
$g_i(x_1, x_2, \ldots, x_n) \; (i = \overline{1,m})$ - обмеження \\
$b_i \; (i = \overline{1,m})$ - фіксовані значення \\
$m$ - кількість обмежень \\
$n$ - кількість параметрів 

У випадку відсутності обмежень маємо задачу безумовної оптимізації.

З курсу вищої математики відомо, що в точці екстремуму (мінімуму, максимуму) нелінійної функції всі її часткові похідні дорівнюють нулю. Отже, для знаходження екстремуму нелінійної функції n змінних необхідно визначити її часткові похідні за всіма змінними і прирівняти їх до нуля. Розв’язок отриманої системи n рівнянь з n невідомими дасть значення змінних, при яких досягається екстремум функції.

Слід зазначити, що точний розв’язок системи рівнянь, в загальному випадку системи нелінійних рівнянь, являє собою досить складне завдання. Тому для відшукання екстремуму нелінійної функції часто використовуються інші методи, зокрема градієнтні методи.

Задачі безумовної мінімізації на практиці зустрічаються рідко, однак методи їхнього розв’язку є основою розв’язку більшості практичних задач умовної оптимізації.

\section {Загальна характеристика методів спуску}

Всі методи розв'язку задачі безумовної оптимізації полягають у побудові послідовність точок $\{x^n\}$ так, щоб послідовність функцій $\phi(x^n)$ була спадною (тобто спуск уздовж функції). На $k$-му кроці $(k > 0)$ визначається вектор $s^k$, в напрямку якого функція $\phi(x^k)$ зменшується. У цьому напрямку робиться крок величиною $\beta_k$ і отримується нова точка
$x^{k+1} = x^k + \beta_ks^k$, в якій $\phi(x^{k+1}) < \phi(x^k)$. Послідовність $\{x^n\}_{n \in \mathbb{N}}$ , що задовольняє цій умові, називається релаксаційною послідовністю, а відповідні методи– методами спуску.

 Методи спуску поділяються на методи із застосуванням інформації про похідні функції і без використання такої. Різні методи спуску відрізняються вибором напрямку і величиною кроку. Як правило, для знаходження $\beta_k$ використовується процедура одновимірного пошуку.

Методи безумовної оптимізації поділяються на:
\begin{itemize}
	\item \textbf{Методи прямого пошуку}\\
	У методах прямого пошуку мінімуму цільової функції (або методах нульового порядку) використовується інформація лише про значення функції. Багато з цих методів не мають строгого теоретичного обґрунтування і побудовані на основі евристичних міркувань. Тому питання збіжності методів прямого пошуку ще мало вивчені, а оцінки швидкості збіжності зазвичай відсутні. Разом із цим дані методи ідейно пов'язані з методами першого і другого порядків, що в ряді випадків дозволяє оцінювати ефективність алгоритмів прямого пошуку стосовно мінімізації деяких класів функцій. Поширеним способом оцінки ефективності методів прямого пошуку є обчислювальні експерименти та порівняльний аналіз методів за результатами таких експериментів.
	 До методів нульового порядку належать методи, які не використовують похідні для вибору напрямку спуска: метод Гауса, метод Хука Дживса, метод обертових напрямків (Розенброка); метод деформованого багатогранника (пошук по симплексу); метод Пауелла.
	\item \textbf{Методи першого порядку}\\
	Методи 1-го порядку використовують інформацію про похідну функції. Якщо обмежена знизу цільова функція $\phi(x), x \in \mathbb{R}^n$ є диференційованою на множині $\mathbb{R}^n$, то алгоритм пошуку точки $x^*$ її мінімуму можна побудувати, використовуючи інформацію, принаймні, про градієнт цієї функції. Такі методи називаються градієнтними. Градієнтні методи безумовної оптимізації використовують тільки перші похідні цільової функції і є методами лінійної апроксимації на кожному кроці, тому, що цільова функція на кожному кроці замінюється дотичною гіперплоскістю до її графіку в поточній точці.
	У всіх цих методах передбачається, що $\phi'(x)$ існують і безперервні. Градієнтні методи розрізняються тільки способом визначення $\beta_k$ і $s^k$. 
	
	До методів першого порядку належать методи: найшвидшого спуску (Коші) та сполучених градієнтів.
	\item \textbf{Методи 2-го порядку (Ньютонівські методи)}\\
	Коли цільова функція $\phi(x)$ двічі диференційована в $\mathbb{R}^n$, то ефективність процесу пошуку точки $x^*$ її мінімуму можна підвищити, використовуючи інформацію не тільки  про градієнт цієї функції, а й про її матрицю Гессе $H(x)$. Напрям пошуку, що відповідає найшвидшому спуску, пов'язаний з лінійною апроксимацією цільової функції. Методи, які використовують інформацію про другі похідні, виникли із квадратичної апроксимації цільової функції $\phi(x)$, яку можна отримати при розкладанні функції в ряд Тейлора 2-го порядку. Мінімум $\phi(x)$ (якщо він існує)  досягається там же, де і мінімум квадратичної форми. \\ 
	Якщо матриця Гессе цільової функції, обчислена в точці $x^k$, є позитивно визначеною, то точка $x^*$ єдина і може бути знайдена з умови, що градієнт функції дорівнює нульовому вектору. Алгоритм оптимізації, в якому напрям пошуку формулюється з цього співвідношення, називається методом Ньютона. \\
	В задачах знаходження мінімуму довільної квадратичної функції із позитивною матрицею других похідних метод Ньютона дає рішення за одну ітерацію незалежно від вибору початкової точки. \\
	До методів 2-го порядку належать: метод Ньютона Рафсона, модифікації методу Ньютона, метод Марквардта.
	\item \textbf{Методи змінної метрики}\\
	Серед алгоритмів багатовимірної мінімізації виділяють групу алгоритмів, що поєднують переваги методів Ньютона та найшвидшого спуску. Дані алгоритми прийнято відносити до так званим квазіньютонівським методам. Особливістю цих алгоритмів є те, що при їх використанні немає необхідності звертати й обчислювати матрицю Гессе цільової функції $\phi(x)$ і у цей же час вдається зберегти високу швидкість збіжності алгоритмів, що притаманна методам Ньютона та його модифікаціям. У цих методах зворотна матриця Гессе апроксимується іншою матрицею – метрикою. Метрика змінюється на кожній ітерації, і тому методи так само називаються методами зі змінною метрикою. \\
	До методів змінної метрики належать наступні методи: Пірсона, Девідона Флетчера Пауелла, Бройдена Флетчера Шенно, Пауелла і Мак Корміка.
	\item \textbf{Методи випадкового пошуку} \\
	Методи випадкового пошуку реалізують ітеративний процес руху оптимізаційних змінних в просторі з використанням випадкових напрямків. Одна з переваг цих методів – достатня простота, методи володіють великим спектром можливих напрямків руху. \\	
	Можливі два алгоритми пошуку. Алгоритм пошуку з лінійною стратегією: визначений напрямок, в якому цільова функція зменшується, не змінюється до тих пір, поки він не призведе до збільшення цільової функції. \\	
	Стратегія лінійного пошуку хороша, коли далеко до оптимуму. Поблизу оптимуму більш доцільна нелінійна стратегія, при якій випадкова зміна напрямків не залежить від результату.
\end{itemize}

\section {Градієнтні методи}

Метод оптимізації, в основі якого лежить ідея руху по самій крутій тропі, називається методом найшвидшого підйому або найшвидшого спуску. Вектор градієнта перпендикулярний лінії рівня і вказує напрямок до нової точки в просторі проектування. Відмітимо, що градієнтний метод на відміну від методу дотичної до лінії рівня можна використовувати до будь-якої унімодальної функції, а не тільки тих, у яких ця властивість явно виражена.

Градієнтні методи прийнято класифікувати насамперед за кількістю використовуваної інформації: для методів першого порядку необхідне значення функції і її перших похідних, методи другого порядку засновані на обчисленні гессіану або його апроксимації.

Методи першого порядку можна розділити на такі підкласи: методи, які не використовують матрицю для апроксимації гессіану, і методи, які послідовно апроксимують гессіан.	



