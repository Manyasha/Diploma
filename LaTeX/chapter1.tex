\chapter{Методи нелінійного програмування без обмежень}

\section {Постановка задачі нелінійного програмування}

У загальному вигляді математична модель задачі нелінійного програмування формулюється наступним чином: знайти вектор , $X = (x_1, x_2, \ldots, x_n) $ при якому б цільова функція набувала свого екстремального (максимального чи мінімального) значення (можливо лише в тому випадку коли цільова функція неперервна, а допустима множина розв'язків замкнена, непуста і обмежена):
$$ \max(\min) \Phi = \phi (x_1, x_2, \ldots, x_n) \eqno(1.1.1) $$
при наступних обмеженнях:
$$ 
\begin{cases}
g_1(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_1 &  \\
g_2(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_2 &  \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots & \\ 
g_m(x_1, x_2, \ldots, x_n)\{\leq, = , \geq \}b_m &  \\
\end{cases}	\eqno(1.1.2)
$$
$$
x_j \geq 0, \; (j = \overline{1,n}) \eqno(1.1.3)
$$
де $ x_j \; (j = \overline{1,n}) $ - змінні \\
$\phi (x_1, x_2, \ldots, x_n)$ - нелінійна функція від $n$ змінних, \\
$g_i(x_1, x_2, \ldots, x_n) \; (i = \overline{1,m})$ - обмеження \\
$b_i \; (i = \overline{1,m})$ - фіксовані значення \\
$m$ - кількість обмежень \\
$n$ - кількість параметрів 

У випадку відсутності обмежень маємо задачу безумовної оптимізації.

З курсу вищої математики відомо, що в точці екстремуму (мінімуму, максимуму) нелінійної функції всі її часткові похідні дорівнюють нулю. Отже, для знаходження екстремуму нелінійної функції n змінних необхідно визначити її часткові похідні за всіма змінними і прирівняти їх до нуля. Розв’язок отриманої системи n рівнянь з n невідомими дасть значення змінних, при яких має досягаться екстремум функції.

Слід зазначити, що точний розв’язок системи рівнянь, в загальному випадку системи нелінійних рівнянь, являє собою досить складне завдання. Тому для пошуку екстремуму нелінійної функції часто використовуються інші методи, зокрема градієнтні методи.

Задачі безумовної мінімізації на практиці зустрічаються рідко, однак методи їхнього розв’язку є основою розв’язку більшості практичних задач умовної оптимізації.

\section {Загальна характеристика методів спуску}

Всі методи розв'язку задачі безумовної оптимізації полягають у побудові послідовністі точок $\{x^n\}$ так, щоб послідовність функцій $\phi(x^n)$ була спадною (тобто спуск уздовж функції). На $k$-му кроці $(k > 0)$ визначається вектор $s^k$, в напрямку якого функція $\phi(x^k)$ зменшується. У цьому напрямку робиться крок величиною $\beta_k$ і отримується нова точка
$x^{k+1} = x^k + \beta_ks^k$, в якій $\phi(x^{k+1}) < \phi(x^k)$. Послідовність $\{x^n\}_{n \in \mathbb{N}}$ , що задовольняє цій умові, називається релаксаційною послідовністю, а відповідні методи – методами спуску.

 Методи спуску поділяються на методи із застосуванням інформації про похідні функції і без використання такої. Різні методи спуску відрізняються вибором напрямку і величиною кроку. Як правило, для знаходження $\beta_k$ використовується процедура одновимірного пошуку. Щодо побудови напрямків спуску розроблено різні підходи, які визначають подальшу класифікацію методів. Зокрема, ті методи, які при побудові напрямків спуску використовують лише інформацію поточної ітерації, називають однокроковими. Якщо використовується додатково інформація попереднього кроку, то двокроковими, і т.д. Класичним двокроковим методом є метод спряжених градієнтів.

Методи безумовної оптимізації також можна поділити на:
\begin{itemize}
	\item \textbf{Методи прямого пошуку}
\end{itemize}

	У методах прямого пошуку мінімуму цільової функції (або методах нульового порядку) використовується інформація лише про значення функції. Багато з цих методів не мають строгого теоретичного обґрунтування і побудовані на основі евристичних міркувань. Тому питання збіжності методів прямого пошуку ще мало вивчені, а оцінки швидкості збіжності зазвичай відсутні. Разом із цим дані методи ідейно пов'язані з методами першого і другого порядків, що в ряді випадків дозволяє оцінювати ефективність алгоритмів прямого пошуку стосовно мінімізації деяких класів функцій. Поширеним способом оцінки ефективності методів прямого пошуку є обчислювальні експерименти та порівняльний аналіз методів за результатами таких експериментів.
	
	 До методів нульового порядку належать методи, які не використовують похідні для вибору напрямку спуска: метод Гауса, метод Хука Дживса, метод обертових напрямків (Розенброка), метод деформованого багатогранника (пошук по симплексу), метод Пауелла.
\begin{itemize}
	\item \textbf{Методи першого порядку}
\end{itemize}
	
	Методи 1-го порядку використовують інформацію про похідну функції. Якщо обмежена знизу цільова функція $\phi(x), x \in \mathbb{R}^n$ є диференційованою на множині $\mathbb{R}^n$, то алгоритм пошуку точки $x^*$ її мінімуму можна побудувати, використовуючи інформацію, принаймні, про градієнт цієї функції. Такі методи називаються градієнтними. Градієнтні методи безумовної оптимізації використовують тільки перші похідні цільової функції і є методами лінійної апроксимації на кожному кроці, тому, що цільова функція на кожному кроці замінюється дотичною гіперплощиною до її графіку в поточній точці.
	У всіх цих методах передбачається, що $\phi'(x)$ існують і неперервні. Градієнтні методи розрізняються тільки способом визначення $\beta_k$ і $s^k$. 
	
	До методів першого порядку належать методи: найшвидшого спуску (Коші) та спряжених градієнтів.
\begin{itemize}	
	\item \textbf{Методи 2-го порядку (Ньютонівські методи)}
\end{itemize}	

	Коли цільова функція $\phi(x)$ двічі диференційована в $\mathbb{R}^n$, то ефективність процесу пошуку точки $x^*$ її мінімуму можна підвищити, використовуючи інформацію не тільки  про градієнт цієї функції, а й про її матрицю Гессе $H(x)$. Напрям пошуку, що відповідає найшвидшому спуску, пов'язаний з лінійною апроксимацією цільової функції. Методи, які використовують інформацію про другі похідні, виникли із квадратичної апроксимації цільової функції $\phi(x)$, яку можна отримати при розкладанні функції в ряд Тейлора 2-го порядку. Мінімум $\phi(x)$ (якщо він існує)  досягається там же, де і мінімум квадратичної форми. 
	
	Якщо матриця Гессе цільової функції, обчислена в точці $x^k$, є додатно визначеною, то точка $x^*$ єдина і може бути знайдена з умови, що градієнт функції дорівнює нульовому вектору. Алгоритм оптимізації, в якому напрям пошуку формулюється з цього співвідношення, називається методом Ньютона. 
	
	В задачах знаходження мінімуму довільної квадратичної функції із додатною матрицею других похідних метод Ньютона дає рішення за одну ітерацію незалежно від вибору початкової точки. 
	
	До методів 2-го порядку належать: метод Ньютона-Рафсона, модифікації методу Ньютона.
\begin{itemize}	
	\item \textbf{Методи змінної метрики}
\end{itemize}

	Серед алгоритмів багатовимірної мінімізації виділяють групу алгоритмів, що поєднують переваги методів Ньютона та найшвидшого спуску. Дані алгоритми прийнято відносити до так званим квазіньютонівським методам. Особливістю цих алгоритмів є те, що при їх використанні немає необхідності обертати й обчислювати матрицю Гессе цільової функції $\phi(x)$ і у цей же час вдається зберегти високу швидкість збіжності алгоритмів, що притаманна методам Ньютона та його модифікаціям. У цих методах обернена матриця Гессе апроксимується іншою матрицею. Метрика змінюється на кожній ітерації, і тому методи так само називаються методами зі змінною метрикою. 
	
	До методів змінної метрики належать наступні методи: Пірсона, Девідона Флетчера Пауелла, Бройдена Флетчера Шенно, Пауелла і Мак Корміка, і інші.
\begin{itemize}
	\item \textbf{Методи випадкового пошуку} 
\end{itemize}

	Методи випадкового пошуку реалізують ітеративний процес руху оптимізаційних змінних в просторі з використанням випадкових напрямків. Одна з переваг цих методів – достатня простота, методи володіють великим спектром можливих напрямків руху. 
		
	Можливі два алгоритми пошуку. Алгоритм пошуку з лінійною стратегією: визначений напрямок, в якому цільова функція зменшується, не змінюється до тих пір, поки він не призведе до збільшення цільової функції. 
		
	Стратегія лінійного пошуку хороша, коли далеко до оптимуму. Поблизу оптимуму більш доцільна нелінійна стратегія, при якій випадкова зміна напрямків не залежить від результату.


\section {Метод спряжених градієнтів}

Розглянемо задачу мінімізації функції багатьох змінних без обмежень
$$ \phi (x) \to min, \; x \in \mathbb{R}^n , \eqno(1.3.1) $$ де  
$$ \phi (x) : \mathbb{R}^n \to \mathbb{R}^1 \text{ - неперервно диференційовна функція } $$
Позначимо її градієнт через $  \phi'(x) $.

Метод спряжених градієнтів має такий загальний вигляд:
$$ x^{k + 1} = x^k + \beta_k s^k, \; k = 0, 1,\ldots $$
$$
S = \quad
\begin{cases}
-\phi' (x^k) & , k = 0 \\
-\phi' (x^k) + \gamma_1^{k-1}s^{k-1} & , k = 1, 2, \ldots
\end{cases}	\eqno(1.3.2)
$$
де $ x^0, x^1, \dotsc , x^k, \dotsc $ - послідовні наближення, \\
$ s^0, s^1, \dotsc , s^k, \dotsc $ - напрямки спуску, \\
$ \beta_k $ - величина кроку вздовж напрямку спуску, \\
$ \gamma_{k-1} $ - числовий параметр 

Одним із можливих варіантів вибору кроку $\beta_k$ є розв'язування задачі одновимірної мінімізації:
$$ \beta_k : \min_{\beta\geq0} \phi(x^k + \beta s^k) \eqno(1.3.3)$$

Практичні дослідження доводять, що якість розв'язку задачі (1.3.1) та швидкість збіжності алгоритму суттєво залежать від якості розв'язку одновимірної задачі (1.3.3).

Різновиди методу (1.3.2) визначаються способом обчислення параметру $\gamma_{k-1}$, зокрема 
\begin{itemize}
	\item $\gamma_{k-1} = \dfrac{\lVert \phi'(x^{k}) \rVert^2}{\lVert \phi'(x^{k-1}) \rVert^2} \text{  (метод Флетчера - Рівза)}$ 
	\item $\gamma_{k-1} = \dfrac{(\phi'(x^{k}), \phi'(x^{k}) - \phi'(x^{k-1}))}{\lVert \phi'(x^{k-1}) \rVert^2} \text{  (метод Полака - Ріб'єра)}$ 
\end{itemize}


\section {Трикроковий метод для задачі багатовимірної оптимізації}

Для задачі нелінійного програмування (1.3.1) трикроковий метод має такий алгоритм[6]: 
$$ x^{k + 1} = x^k + \beta_k s^k, \; k = 0, 1,\ldots $$
$$
S = \quad
\begin{cases}
-\phi' (x^k) & , k = 0 \\
-\phi' (x^k) + \gamma_1^{k-1}s^{k-1} & , k = 1 \\
-\phi' (x^k) + \gamma_1^{k-1}s^{k-1} + \gamma_2^{k-2}s^{k-2} & , k = 2, \ldots
\end{cases}	\eqno(1.4.1)
$$
де $ x^0, x^1, \dotsc , x^k, \dotsc $ - послідовні наближення \\
$ s^0, s^1, \dotsc , s^k, \dotsc $ - напрямки спуску \\
$ \beta_k, \gamma_{k-1}, \gamma_{k-2} $ - числові параметри 


